{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talapas knowledge base  \n",
    "https://hpcrcf.atlassian.net/wiki/spaces/TCP/overview?homepageId=6761503  \n",
    "Service Desk  \n",
    "https://hpcrcf.atlassian.net/servicedesk/customer/portal/1  \n",
    "Before you read the rest of this notebook, go read the Talapas Quick Start Guide:  \n",
    "https://hpcrcf.atlassian.net/wiki/spaces/TCP/pages/7312376/Quick+Start+Guide  \n",
    "There's a lot of information there, including how to request an account. This notebook is a companion to that guide, which I'll be refering to frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging in\n",
    "You log into talapas on a login node, but you shouldn't do any serious work there! As the Quick Start guide states: \"The login nodes are for light tasks needed to set up and submit your work.  They're not for running significant applications, simulations, etc.  Processes that use a lot of memory or CPU will be killed.\" There are two login nodes, ln1 and ln2. You can ssh into talapas with either:  \n",
    "  \n",
    "ssh username@talapas-ln1.uoregon.edu  \n",
    "ssh username@talapas-ln2.uoregon.edu  \n",
    "ssh username@talapas-login.uoregon.edu  \n",
    "  \n",
    "You will need to be connected to the on campus network (directly or with VPN) for that to work. If you want to run matlab, fsleyes, or some other graphical interface, detailed instructions are here:  \n",
    "  \n",
    "https://hpcrcf.atlassian.net/wiki/spaces/TCP/pages/7312376/Quick+Start+Guide#QuickStartGuide-RunningGraphicalInteractiveJobs  \n",
    "  \n",
    "The easiest way to do this is to use OnDemand. In my experience, it also has the best performance when logging in from home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OnDemand\n",
    "Assuming you already have a talapas account, the easiest way to log in is to use OnDemand. No VPN required.    \n",
    "Open a private browser window and go to:  \n",
    "https://talapas-ln1.uoregon.edu/  \n",
    "OR  \n",
    "https://talapas-ln2.uoregon.edu/  \n",
    "Log in using your Duck ID  \n",
    "\n",
    "![](1.images/ondemand.png)\n",
    "\n",
    "Menu options include:  \n",
    "* Files: opens a file browser. You can download and upload files here.\n",
    "* Jobs: view your active jobs, or use the Job Composer to create a job script. We'll come back to this later.\n",
    "* Clusters: This will let you work on a login node without starting a job or interactive app, similar to ssh. You won't have access to any graphical interface, and you shouldn't use this for anything but light tasks.\n",
    "* Interactive apps: Start an interactive app. This is where you'll do your \"real work.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directories on talapas\n",
    "* your home directory. This is where you start when you log in. There's very little space for files here! (10 GB) Keep small text files nad scripts here but data elsewhere.  \n",
    "* /projects/[pirg]/USERNAME  This is your project directory and where you should store data that you don't share with other pirg members. It shares a quota with your pirg. pirg stands for primary investigator research group, and is usually your lab's name (for me, it's lcni). If you aren't sure what yours is, type `groups` at a command prompt. You should be in at least two: the talapas group and your pirg.\n",
    "* /projects/[pirg]/shared  This also shares a quota with your pirg, and is intended for data that you share with other pirg members.\n",
    "* /tmp  This directory is for temporary files. It should be faster than using a directory inside the /projects paths. If you use this directory, be a good citizen and delete any files you create when you are through with them. This directory is not shared across hosts, so if you use it for one job another job will not be able to access it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permissions and access control lists\n",
    "You can use access control lists to give or restrict access to your files. To see who has access, use the command getfacl. For example, if you run `getfacl /project/[pirg]/shared`, you should see something like this:  \n",
    "```\n",
    "[jolinda@talapas-ln1]$ getfacl /project/lcni/shared\n",
    "# file: projects/lcni/shared  \n",
    "# owner: fws\n",
    "# group: lcni \n",
    "# flags: -s-  \n",
    "user::rwx  \n",
    "group::rwx  \n",
    "other::---\n",
    "```\n",
    "\n",
    "There are three levels of permissions (user, group, other), and three types of permissions (read, write, and execute). \"Owner\" shows the default user, and is usually the person who created the file or folder. \"Group\" shows the default group, usually your pirg. \"Other\" covers everyone else. In this example, anyone in the pirg can read, write, or execute any file in the shared folder, while those outside the pirg have no access. What happens if you create a folder inside this directory?\n",
    "\n",
    "```\n",
    "[jolinda@talapas-ln1]$ cd /projects/lcni/shared\n",
    "[jolinda@talapas-ln1 shared]$ mkdir test\n",
    "[jolinda@talapas-ln1 shared]$ getfacl test\n",
    "# file: test\n",
    "# owner: jolinda\n",
    "# group: lcni\n",
    "# flags: -s-\n",
    "user::rwx\n",
    "group::r-x\n",
    "other::r-x\n",
    "```\n",
    "\n",
    "You have read/write/execute permissions in the folder. Everyone else has read & execute permissions. Because they don't have 'x' permissions on the parent folder, users outside of your pirg still won't be able to access this folder. You can change permissions using the `setfacl` command (as long as you are the owner). For instance, suppose you don't want most members of your pirg to be able to read the folder, but you do want user janedoe to have full permissions:\n",
    "\n",
    "```\n",
    "[jolinda@talapas-ln1 shared]$ setfacl -m group::--- test\n",
    "[jolinda@talapas-ln1 shared]$ setfacl -m user:janedoe:rwx test\n",
    "[jolinda@talapas-ln1 shared]$ getfacl test\n",
    "# file: test\n",
    "# owner: jolinda\n",
    "# group: lcni\n",
    "# flags: -s-\n",
    "user::rwx\n",
    "user:janedoe:rwx\n",
    "group::---\n",
    "mask::rwx\n",
    "other::r-x\n",
    "```\n",
    "Again, if janedoe is not a member of your pirg, these permissions won't do her much good because she has no permissions for /projects/[pirg]/shared. Yes, this makes sharing data with members of other pirgs a bit of a pain unless you are the owner of /projects/[pirg], in which case you have all the permissions and can do what you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitions\n",
    "When starting a job on Talapas, you'll need to specify what partition you want it to run on. If your pirg has it's own condo partition, you'll probably want to use it. Otherwise, you'll probably be choosing between interactive, short, long, and gpu. The biggest difference between these is how long you have before being kicked off:\n",
    "* interactive: 4 hours\n",
    "* short: 24 hours\n",
    "* long: 14 days\n",
    "* gpu: 24 hours but you can use gpus\n",
    "There are more partitions, including some with access to more memory. See the full list here: https://hpcrcf.atlassian.net/wiki/spaces/TCP/pages/7285967/Partition+List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Apps \n",
    "There are currently two: Talapas Desktop and Jupyter Notebook. For either, you'll need the following information: your pirg, the partition you are requesting, and how long you want your job to run. You may also want to include 'idx: [project/index]' in the comment field to help with billing. \n",
    "\n",
    "Talapas desktop gives you a nice desktop environment where you can run your guis. Some applications including matlab and rstudio can be launched from the menu. For others you'll need to load a module, which we'll cover next.  \n",
    "  \n",
    "![](1.images/desktop.png)  \n",
    "Jupyter Notebook will start a JupyterLab session with an interactive python kernel. There are several options for what sort of notebook to start, most of which are for specific UO courses. I will be using the Python3/TensorFlow server for my examples. We'll be coming back to Jupyter Notebook later (it's where this document was created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules  \n",
    "Most software on Talapas is installed as LMOD modules. A nice guide on how to use them is here:  \n",
    "  \n",
    "https://hpcrcf.atlassian.net/wiki/spaces/TCP/pages/7198035/How-to+Use+LMOD  \n",
    "\n",
    "To see all available modules, use module avail, or module avail [name]. For example, to see all available modules starting with fsl:\n",
    "```\n",
    "[jolinda@talapas-ln1 ~]$ module avail fsl\n",
    "\n",
    "----------------------------------- /packages/modulefiles/Core ----------------------------------\n",
    "   fsl/5.0.9    fsl/5.0.10 (D)    fsl/6.0.1    fsleyes/0.23.0\n",
    "\n",
    "  Where:\n",
    "   D:  Default Module\n",
    "\n",
    "Use \"module spider\" to find all possible modules.\n",
    "Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n",
    "```\n",
    "We see that there are four packages matching \"fsl\", and the default module is version 5.0.10. That's the version that will load if you simply type `module load fsl`. To control which version of fsl you load, type the whole name, eg `module load fsl/6.0.1`. Once the module is loaded you'll be able to run any of the usual commands from the command line.  \n",
    "  \n",
    "'module spider' returns similar information to 'module avail', except that it doesn't indicate the default module and will also show modules that can't be loaded with 'module load'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slurm\n",
    "Slurm Workload Manager is an open source job scheduler, formerly known as the Simple Linux Utility for Resource Management. When you submit a job to the talapas cluster, Slurm is the software that decides where and when to send it and keeps track of whether it completes successfully or not. You can see all the active jobs for just yourself or for all users by choosing the \"active jobs\" menu option, or by using the command `sacct`. `sacct` will also show you completed or canceled jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n",
      "------------ ---------- ---------- ---------- ---------- ---------- -------- \n",
      "11691589     sys/dashb+      short       lcni          1    TIMEOUT      0:0 \n",
      "11691589.ba+      batch                  lcni          1  CANCELLED     0:15 \n",
      "11691589.ex+     extern                  lcni          1  COMPLETED      0:0 \n",
      "11745141     sys/dashb+      short       lcni          1    RUNNING      0:0 \n",
      "11745141.ba+      batch                  lcni          1    RUNNING      0:0 \n",
      "11745141.ex+     extern                  lcni          1    RUNNING      0:0 \n",
      "11745178     sys/dashb+      short       lcni          1    RUNNING      0:0 \n",
      "11745178.ba+      batch                  lcni          1    RUNNING      0:0 \n",
      "11745178.ex+     extern                  lcni          1    RUNNING      0:0 \n",
      "11745191        convert      short       lcni          1  COMPLETED      0:0 \n",
      "11745191.ba+      batch                  lcni          1  COMPLETED      0:0 \n",
      "11745191.ex+     extern                  lcni          1  COMPLETED      0:0 \n",
      "11745192        convert      short       lcni          1  COMPLETED      0:0 \n",
      "11745192.ba+      batch                  lcni          1  COMPLETED      0:0 \n",
      "11745192.ex+     extern                  lcni          1  COMPLETED      0:0 \n"
     ]
    }
   ],
   "source": [
    "!sacct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'sacct' shows recent job information for the current user. Each job has a JobID and a name. Jobs may have separate steps (indicated by jobId.something), with their own names. The '+' signs mean the names are too long for the default column widths. We can use the -j parameter to show the result of a specific jobid (it can be another user's job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n",
      "------------ ---------- ---------- ---------- ---------- ---------- -------- \n",
      "11617266     snakejob.+      short    kernlab          1  COMPLETED      0:0 \n",
      "11617266.ba+      batch               kernlab          1  COMPLETED      0:0 \n",
      "11617266.ex+     extern               kernlab          1  COMPLETED      0:0 \n"
     ]
    }
   ],
   "source": [
    "!sacct -j 11617266 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the format command to change which columns are shown. There are a ton of columns you can include in the format command, and there's a special command to list them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account             AdminComment        AllocCPUS           AllocGRES          \n",
      "AllocNodes          AllocTRES           AssocID             AveCPU             \n",
      "AveCPUFreq          AveDiskRead         AveDiskWrite        AvePages           \n",
      "AveRSS              AveVMSize           BlockID             Cluster            \n",
      "Comment             Constraints         ConsumedEnergy      ConsumedEnergyRaw  \n",
      "CPUTime             CPUTimeRAW          DerivedExitCode     Elapsed            \n",
      "ElapsedRaw          Eligible            End                 ExitCode           \n",
      "Flags               GID                 Group               JobID              \n",
      "JobIDRaw            JobName             Layout              MaxDiskRead        \n",
      "MaxDiskReadNode     MaxDiskReadTask     MaxDiskWrite        MaxDiskWriteNode   \n",
      "MaxDiskWriteTask    MaxPages            MaxPagesNode        MaxPagesTask       \n",
      "MaxRSS              MaxRSSNode          MaxRSSTask          MaxVMSize          \n",
      "MaxVMSizeNode       MaxVMSizeTask       McsLabel            MinCPU             \n",
      "MinCPUNode          MinCPUTask          NCPUS               NNodes             \n",
      "NodeList            NTasks              Priority            Partition          \n",
      "QOS                 QOSRAW              Reason              ReqCPUFreq         \n",
      "ReqCPUFreqMin       ReqCPUFreqMax       ReqCPUFreqGov       ReqCPUS            \n",
      "ReqGRES             ReqMem              ReqNodes            ReqTRES            \n",
      "Reservation         ReservationId       Reserved            ResvCPU            \n",
      "ResvCPURAW          Start               State               Submit             \n",
      "Suspended           SystemCPU           SystemComment       Timelimit          \n",
      "TimelimitRaw        TotalCPU            TRESUsageInAve      TRESUsageInMax     \n",
      "TRESUsageInMaxNode  TRESUsageInMaxTask  TRESUsageInMin      TRESUsageInMinNode \n",
      "TRESUsageInMinTask  TRESUsageInTot      TRESUsageOutAve     TRESUsageOutMax    \n",
      "TRESUsageOutMaxNode TRESUsageOutMaxTask TRESUsageOutMin     TRESUsageOutMinNode\n",
      "TRESUsageOutMinTask TRESUsageOutTot     UID                 User               \n",
      "UserCPU             WCKey               WCKeyID             WorkDir            \n"
     ]
    }
   ],
   "source": [
    "!sacct --helpformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completed jobs, \"Elapsed\"and \"MaxRSS\" are particularly helpful. They tell us how long the job ran, and how much memory it used. This can be useful for planning how much time & memory to request in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Elapsed     MaxRSS     ReqMem      User    Account  Partition \n",
      "---------- ---------- ---------- --------- ---------- ---------- \n",
      "  00:11:44                 220Gc   jadrion    kernlab      short \n",
      "  00:11:44 107742656K      220Gc              kernlab            \n",
      "  00:11:44          0      220Gc              kernlab            \n"
     ]
    }
   ],
   "source": [
    "!sacct -j 11617266 --format=\"Elapsed, MaxRSS, ReqMem, user, account, partition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job used 107G of ram, and requested 220G, and took 11 minutes 44 seconds to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a job: sbatch\n",
    "You launch jobs from the command line with the sbatch command. Very simple jobs can be launched using the --wrap keyword. More complex jobs can be launched from a saved file. In these examples I'll be using my pirg (lcni) but you should use yours. Since I'm in a notebook, I'm starting all my bash commands with and exclamation point. Don't use these if you are in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11749761\n"
     ]
    }
   ],
   "source": [
    "!sbatch --account=lcni --wrap \"echo hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n",
      "------------ ---------- ---------- ---------- ---------- ---------- -------- \n",
      "11749761           wrap      short       lcni          1  COMPLETED      0:0 \n",
      "11749761.ba+      batch                  lcni          1  COMPLETED      0:0 \n",
      "11749761.ex+     extern                  lcni          1  COMPLETED      0:0 \n"
     ]
    }
   ],
   "source": [
    "!sacct -j 11749761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-11749761.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a more descriptive name than \"wrap\", we can set that. There are lots of options you can set, there's a convenient list of them here: https://slurm.schedmd.com/pdfs/summary.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11749773\n"
     ]
    }
   ],
   "source": [
    "!sbatch --account=lcni --job-name='echo' --wrap \"echo hello\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n",
      "------------ ---------- ---------- ---------- ---------- ---------- -------- \n",
      "11749773           echo      short       lcni          1  COMPLETED      0:0 \n",
      "11749773.ba+      batch                  lcni          1  COMPLETED      0:0 \n",
      "11749773.ex+     extern                  lcni          1  COMPLETED      0:0 \n"
     ]
    }
   ],
   "source": [
    "!sacct -j 11749773"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases it's easier to write a script file and submit that. Here's a simple one:\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=fslinfo\n",
    "#SBATCH --account=lcni\n",
    "\n",
    "module load fsl/6.0.1\n",
    "fslinfo bold.nii.gz\n",
    "```  \n",
    "If I save that text to a file called fslinfo.srun, I can submit it to slurm with `sbatch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11749811\n"
     ]
    }
   ],
   "source": [
    "!sbatch fslinfo.srun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n",
      "------------ ---------- ---------- ---------- ---------- ---------- -------- \n",
      "11749811        fslinfo      short       lcni          1  COMPLETED      0:0 \n",
      "11749811.ba+      batch                  lcni          1  COMPLETED      0:0 \n",
      "11749811.ex+     extern                  lcni          1  COMPLETED      0:0 \n"
     ]
    }
   ],
   "source": [
    "!sacct -j 11749811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type\tINT16\n",
      "dim1\t\t64\n",
      "dim2\t\t64\n",
      "dim3\t\t30\n",
      "dim4\t\t184\n",
      "datatype\t4\n",
      "pixdim1\t\t4.000000\n",
      "pixdim2\t\t4.000000\n",
      "pixdim3\t\t3.999975\n",
      "pixdim4\t\t2.500000\n",
      "cal_max\t\t0.000000\n",
      "cal_min\t\t0.000000\n",
      "file_type\tNIFTI-1+\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-11749811.out # cat is a bash command that prints a file to the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of a slurm script  \n",
    "  \n",
    "Let's take apart that script line by line. First we define the interpreter. You'll probably usually want bash, but you can use other scripting languages such as python. Just make sure you have the right path  \n",
    "`#!/bin/bash`   \n",
    "  \n",
    "Next up are our SLURM options, one per line. Account is the only required one (and that's only if you don't have environment variables set)  \n",
    "`#SBATCH --job-name=fslinfo`   \n",
    "`#SBATCH --account=lcni` \n",
    "   \n",
    "Finally, we have our bash commands:  \n",
    "`module load fsl/6.0.1`  \n",
    "`fslinfo bold.nii.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Array jobs  \n",
    "We often need to run the same command on a long list of subjects. This is when array jobs come in handy. Instead of writing 10 different scripts with different subject names, we write one script with an array of names. Just separate out the part of the command that changes and replace it with ${x}, set a bash array variable named x, and include the array parameter in your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "#SBATCH --job-name=hello_array\n",
      "#SBATCH --account=lcni\n",
      "#SBATCH --array=0-3\n",
      "\n",
      "data=(Eugene Oregon USA World)\n",
      "\n",
      "x=${data[$SLURM_ARRAY_TASK_ID]}\n",
      "\n",
      "echo hello ${x}\n"
     ]
    }
   ],
   "source": [
    "!cat hello_everyone.srun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 20940806\n"
     ]
    }
   ],
   "source": [
    "!sbatch hello_everyone.srun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               JobID                   JobName  Partition      State    Elapsed     MaxRSS \n",
      "-------------------- ------------------------- ---------- ---------- ---------- ---------- \n",
      "          20940806_0               hello_array      short  COMPLETED   00:00:02            \n",
      "    20940806_0.batch                     batch             COMPLETED   00:00:02          0 \n",
      "   20940806_0.extern                    extern             COMPLETED   00:00:02          0 \n",
      "          20940806_1               hello_array      short  COMPLETED   00:00:02            \n",
      "    20940806_1.batch                     batch             COMPLETED   00:00:02          0 \n",
      "   20940806_1.extern                    extern             COMPLETED   00:00:02          0 \n",
      "          20940806_2               hello_array      short  COMPLETED   00:00:02            \n",
      "    20940806_2.batch                     batch             COMPLETED   00:00:02          0 \n",
      "   20940806_2.extern                    extern             COMPLETED   00:00:02          0 \n",
      "          20940806_3               hello_array      short  COMPLETED   00:00:02            \n",
      "    20940806_3.batch                     batch             COMPLETED   00:00:02          0 \n",
      "   20940806_3.extern                    extern             COMPLETED   00:00:02          0 \n"
     ]
    }
   ],
   "source": [
    "!sacct -j 20940806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created a single job with four tasks. For each task, the variable ${x} was replaced with the corresponding value in the array \"data\". There's a slurm output file for each value of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello Eugene\n",
      "hello Oregon\n",
      "hello USA\n",
      "hello World\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-20940806_*.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slurm parameters to know (with examples)\n",
    "`--partition=long`  \n",
    "Partition you are submitting to. The default is short.  \n",
    "`--output=slurmjobname.out`  \n",
    "`--error=slurmjobname-%j.err`  \n",
    "Alternative to the slurm-{jobnumber}.out filename for standard out and standard error. %j will be replaced with the jobnumber  \n",
    "`--time=5`  \n",
    "`--time=5:30:0`  \n",
    "`--time=2-0`  \n",
    "Time limit for your job. First example is 5 minutes, second is 5 hours 30 minutes (h:m:s), third is 2 days (d-h). If unspecified it will be the limit for the partition (24 hours for short, 14 days for long, etc). Asking for less time means your job should launch more quickly.   \n",
    "`--mem=16G`  \n",
    "Memory requested for your job. The default amount varies, but it's about 4 GB for a standard node. This will not be enough for some jobs. You'll know you need more memory because your job will fail. Don't just ask for all the memory on the node (128 GB). It will take longer to launch your job and you will be taking resources away from other users. If you are doing something that requires more then 128 GB, use fat or longfat.  \n",
    "`--cpus-per-task=1`  \n",
    "Number of threads per task. If you are running something that can take advantage of multithreading, increase this number.  \n",
    "`--dependency=afterok:123456`  \n",
    "Run this job after job 123456 finishes with no errors  \n",
    "`--mail-user=email_address -mail-type=END `  \n",
    "Send an email when the job ends. I like to combine this with email-to-text from my carrier to get a text messsage notification.  \n",
    "`--comment=idx:index`  \n",
    "Write a comment. Talapas may use idx:index to indicate which project should be billed for your time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canceling a job\n",
    "You can cancel a job with the scancel command: `scancel jobnumber`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
